# test
ret为可执行文件, ret file1 file2是从file1中获得数据，将结果打印到file2.<br>
整体思想是先将文件中的url按照哈希值膜一个值的余数分成若干个文件，在每个文件中用trie树统计不同url对应出现的次数。同时维护一个小根堆来实现获得出现次数在前100的url操作。<br>
在最极端的情况下，所有的url都被分到同一个文件中，这样内存就爆炸了，可以再采取不同的哈希和膜值再细分文件，使得每个文件中的url种数减小，但是并没有实现= =。<br>
空间的瓶颈在一个文件中最多的url种数，如果url完全随机，各个文件中分布应该是比较均匀的，是O(N/K)，K是模数，但最差是O(N)。<br>
时间的瓶颈是IO和小根堆的维护，小根堆的部分是O(NlogM)的，但IO应该更费时间。<br>
因为会产生同等大小的额外的磁盘文件，很捞的只测试了几M的小测试用例...<br>